
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Logistic</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-08-04"><meta name="m-file" content="Logistic"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">LOGISTIC implements Bayesian inference with a logistic model:</a></li><li><a href="#3">LOGISTIC (ctor)</a></li><li><a href="#5">Car Experiment Data</a></li><li><a href="#6">Logistic Regression Model</a></li><li><a href="#9">Slice Sampling</a></li><li><a href="#10">Analysis of Sampler Output</a></li><li><a href="#16">Inference for the Model Parameters</a></li></ul></div><pre class="codeinput"><span class="keyword">classdef</span> Logistic &lt; mlentropy.Bayesian
</pre><h2>LOGISTIC implements Bayesian inference with a logistic model:<a name="2"></a></h2><pre class="codeinput">    <span class="comment">%</span>
    <span class="comment">%  $$f(z) = \frac{e^z}{1 + e^z}, z \equiv B_0 + B_1 x_1 + B_2 x_2 + ...$$</span>
    <span class="comment">%  This is a generalized linear model.</span>
    <span class="comment">%</span>
	<span class="comment">%  Version $Revision$ was created $Date$ by $Author$</span>
 	<span class="comment">%  and checked into svn repository $URL$</span>
 	<span class="comment">%  Developed on Matlab 7.10.0.499 (R2010a)</span>
 	<span class="comment">%  $Id$</span>
 	<span class="comment">%  N.B. classdef (Sealed, Hidden, InferiorClasses = {?class1,?class2}, ConstructOnLoad)</span>

	properties
 		<span class="comment">% N.B. (Abstract, Access='private', GetAccess='protected', SetAccess='protected', ...</span>
 		<span class="comment">%       Constant, Dependent, Hidden, Transient)</span>
        samples     <span class="comment">% n x k   numeric arrays</span>
        outcomes    <span class="comment">% n x 1   numeric col. vector</span>
        priors      <span class="comment">% cell of function handles</span>
        posterior   <span class="comment">%         function handle</span>
        model       <span class="comment">%         function handle</span>
        modelName = <span class="string">'logistic'</span>
        modelParams <span class="comment">% cell of B0, B1, B2, ...</span>
    <span class="keyword">end</span>

    properties (Dependent)
        nsamples
    <span class="keyword">end</span>

	methods

 		<span class="keyword">function</span> this = Logistic(sample, outcome)
</pre><h2>LOGISTIC (ctor)<a name="3"></a></h2><pre>Usage:  obj = Logistic(samples, binaryoutcomes)</pre><pre class="codeinput">			<span class="comment">%                         ^        ^ numeric, dip_image, NIfTI, NIfTI filenames or cells of the same</span>
			<span class="comment">%                         n x k      n x 1</span>
            this = this@mlentropy.Bayesian;
            import <span class="string">mlfourd.*</span>;
            <span class="keyword">switch</span> (class(sample))
                <span class="keyword">case</span> <span class="string">'char'</span>
                    pnii = ensureNii(sample);
                    pdat = pnii.img;
                <span class="keyword">case</span> NIfTI.NIFTI_SUBCLASS
                    pdat = sample.img;
                <span class="keyword">case</span> NIfTI.NUMERIC_TYPES
                    pdat = double(sample);
                <span class="keyword">otherwise</span>
                    paramError(<span class="string">'mlentropy.Logistic.ctor'</span>, <span class="string">'class(sample)'</span>, class(sample));
            <span class="keyword">end</span>
            this.samples = pdat;
            <span class="keyword">switch</span> (class(outcome))
                <span class="keyword">case</span> <span class="string">'char'</span>
                    onii = ensureNii(outcome);
                    odat = onii.img;
                <span class="keyword">case</span> NIfTI.NIFTI_SUBCLASS
                    odat = outcome.img;
                <span class="keyword">case</span> NIfTI.NUMERIC_TYPES
                    odat = double(outcome);
                <span class="keyword">otherwise</span>
                    paramError(<span class="string">'mlentropy.Logistic.ctor'</span>, <span class="string">'class(outcome)'</span>, class(outcome));
            <span class="keyword">end</span>
            this.outcomes = odat;
            rand(<span class="string">'state'</span>,0); randn(<span class="string">'state'</span>,0);
</pre><pre class="codeoutput">Input argument "sample" is undefined.

Error in ==&gt; Logistic&gt;Logistic.Logistic at 38
            switch (class(sample))
</pre><pre class="codeinput"> 		<span class="keyword">end</span> <span class="comment">% Logistic (ctor)</span>

        <span class="keyword">function</span> this = set.samples(this, s)
            this.samples = ensureNumeric(s, <span class="string">'single'</span>);
        <span class="keyword">end</span>

        <span class="keyword">function</span> n = get.nsamples(this)
            n = size(this.samples,1);
        <span class="keyword">end</span>
 	<span class="keyword">end</span>

	methods (Static)
 		<span class="comment">% N.B. (Static, Abstract, Access=', Hidden, Sealed)</span>
        <span class="keyword">function</span> demo
</pre><h2>Car Experiment Data<a name="5"></a></h2><p>In some simple problems such as the previous normal mean inference example, it is easy to figure out the posterior distribution in a closed form. But in general problems that involve non-conjugate priors, the posterior distributions are difficult or impossible to compute analytically. We will consider logistic regression as an example. This example involves an experiment to help model the proportion of cars of various weights that fail a mileage test. The data include observations of weight, number of cars tested, and number failed.  We will work with a transformed version of the weights to reduce the correlation in our estimates of the regression parameters.</p><pre class="codeinput">            <span class="comment">% A set of car weights</span>
            weight = [2100 2300 2500 2700 2900 3100 3300 3500 3700 3900 4100 4300]';
            weight = (weight-2800)/1000;     <span class="comment">% recenter and rescale</span>
            <span class="comment">% The number of cars tested at each weight</span>
            total = [48 42 31 34 31 21 23 23 21 16 17 21]';
            <span class="comment">% The number of cars that have poor mpg performances at each weight</span>
            poor = [1 2 0 3 8 8 14 17 19 15 17 21]';

            obj = mlentropy.Logistic(weight, poor ./ total);
</pre><h2>Logistic Regression Model<a name="6"></a></h2><p>Logistic regression, a special case of a generalized linear model, is appropriate for these data since the response variable is binomial. The logistic regression model can be written as:</p><p><img src="Logistic_eq47865.png" alt="$$P(\mathrm{failure}) = \frac{e^{Xb}}{1+e^{Xb}}$$"></p><p>where X is the design matrix and b is the vector containing the model parameters. In MATLAB&reg;, we can write this equation as:</p><pre class="codeinput">            obj.model = @(b,x) exp(b(1)+b(2).*x)./(1+exp(b(1)+b(2).*x));
</pre><p>If you have some prior knowledge or some non-informative priors are available, you could specify the prior probability distributions for the model parameters. For example, in this demo, we use normal priors for the intercept <tt>b1</tt> and slope <tt>b2</tt>, i.e.</p><pre class="codeinput">            obj.priors    = cell(1,2);
            obj.priors{1} = @(b1) normpdf(b1,0,20);    <span class="comment">% prior for intercept</span>
            obj.priors{2} = @(b2) normpdf(b2,0,20);    <span class="comment">% prior for slope</span>
</pre><p>By Bayes' theorem, the joint posterior distribution of the model parameters is proportional to the product of the likelihood and priors.</p><pre class="codeinput">            obj.posterior = @(b) prod(binopdf(poor,total,obj.model(b,weight))) <span class="keyword">...</span><span class="comment">  % likelihood</span>
                                 * obj.priors{1}(b(1)) * obj.priors{2}(b(2));       <span class="comment">% priors</span>
</pre><h2>Slice Sampling<a name="9"></a></h2><p>Monte Carlo methods are often used in Bayesian data analysis to summarize the posterior distribution. The idea is that, even if you cannot compute the posterior distribution analytically, you can generate a random sample from the distribution and use these random values to estimate the posterior distribution or derived statistics such as the posterior mean, median, standard deviation, etc. Slice sampling is an algorithm designed to sample from a distribution with an arbitrary density function, known only up to a constant of proportionality -- exactly what is needed for sampling from a complicated posterior distribution whose normalization constant is unknown. The algorithm does not generate independent samples, but rather a Markovian sequence whose stationary distribution is the target distribution.  Thus, the slice sampler is a Markov Chain Monte Carlo (MCMC) algorithm.  However, it differs from other well-known MCMC algorithms because only the scaled posterior need be specified -- no proposal or marginal distributions are needed.</p><p>This example shows how to use the slice sampler as part of a Bayesian analysis of the mileage test logistic regression model, including generating a random sample from the posterior distribution for the model parameters, analyzing the output of the sampler, and making inferences about the model parameters.  The first step is to generate a random sample.</p><pre class="codeinput">            initial = [1 1];
            nsamples = 1000;
            trace = slicesample(initial,nsamples,<span class="string">'pdf'</span>,obj.posterior,<span class="string">'width'</span>,[20 2]);
</pre><h2>Analysis of Sampler Output<a name="10"></a></h2><p>After obtaining a random sample from the slice sampler, it is important to investigate issues such as convergence and mixing, to determine whether the sample can reasonably be treated as a set of random realizations from the target posterior distribution. Looking at marginal trace plots is the simplest way to examine the output.</p><pre class="codeinput">            subplot(2,1,1)
            plot(trace(:,1))
            ylabel(<span class="string">'Intercept'</span>);
            subplot(2,1,2)
            plot(trace(:,2))
            ylabel(<span class="string">'Slope'</span>);
            xlabel(<span class="string">'Sample Number'</span>);
</pre><p>It is apparent from these plots is that the effects of the parameter starting values take a while to disappear (perhaps fifty or so samples) before the process begins to look stationary.</p><p>It is also be helpful in checking for convergence to use a moving window to compute statistics such as the sample mean, median, or standard deviation for the sample.  This produces a smoother plot than the raw sample traces, and can make it easier to identify and understand any non-stationarity.</p><pre class="codeinput">            movavg = filter( (1/50)*ones(50,1), 1, trace);
            subplot(2,1,1)
            plot(movavg(:,1))
            xlabel(<span class="string">'Number of samples'</span>)
            ylabel(<span class="string">'Means of the intercept'</span>);
            subplot(2,1,2)
            plot(movavg(:,2))
            xlabel(<span class="string">'Number of samples'</span>)
            ylabel(<span class="string">'Means of the slope'</span>);
</pre><p>Because these are moving averages over a window of 50 iterations, the first 50 values are not comparable to the rest of the plot.  However, the remainder of each plot seems to confirm that the parameter posterior means have converged to stationarity after 100 or so iterations.  It is also apparent that the two parameters are correlated with each other, in agreement with the earlier plot of the posterior density.</p><p>Since the settling-in period represents samples that can not reasonably be treated as random realizations from the target distribution, it's probably advisable not to use the first 50 or so values at the beginning of the slice sampler's output.  You could just delete those rows of the output, however, it's also possible to specify a "burn-in" period.  This is convenient when a suitable burn-in length is already known, perhaps from previous runs.</p><pre class="codeinput">            trace = slicesample(initial,nsamples,<span class="string">'pdf'</span>,obj.posterior, <span class="keyword">...</span>
                                                 <span class="string">'width'</span>,[20 2],<span class="string">'burnin'</span>,50);
            subplot(2,1,1)
            plot(trace(:,1))
            ylabel(<span class="string">'Intercept'</span>);
            subplot(2,1,2)
            plot(trace(:,2))
            ylabel(<span class="string">'Slope'</span>);
</pre><p>These trace plots do not seem to show any non-stationarity, indicating that the burn-in period has done its job.</p><p>However, there is a second aspect of the trace plots that should also be explored.  While the trace for the intercept looks like high frequency noise, the trace for the slope appears to have a lower frequency component, indicating there autocorrelation between values at adjacent iterations.  We could still compute the mean from this autocorrelated sample, but it is often convenient to reduce the storage requirements by removing redundancy in the sample.  If this eliminated the autocorrelation, it would also allow us to treat this as a sample of independent values.  For example, you can thin out the sample by keeping only every 10th value.</p><pre class="codeinput">            trace = slicesample(initial,nsamples,<span class="string">'pdf'</span>,obj.posterior,<span class="string">'width'</span>,[20 2], <span class="keyword">...</span>
                                                            <span class="string">'burnin'</span>,50,<span class="string">'thin'</span>,10);
</pre><p>To check the effect of this thinning, it is useful to estimate the sample autocorrelation functions from the traces and use them to check if the samples mix rapidly.</p><pre class="codeinput">            F    =  fft(detrend(trace,<span class="string">'constant'</span>));
            F    =  F .* conj(F);
            ACF  =  ifft(F);
            ACF  =  ACF(1:21,:);                          <span class="comment">% Retain lags up to 20.</span>
            ACF  =  real([ACF(1:21,1) ./ ACF(1,1) <span class="keyword">...</span>
                         ACF(1:21,2) ./ ACF(1,2)]);       <span class="comment">% Normalize.</span>
            bounds  =  sqrt(1/nsamples) * [2 ; -2];       <span class="comment">% 95% CI for iid normal</span>

            labs = {<span class="string">'Sample ACF for intercept'</span>,<span class="string">'Sample ACF for slope'</span> };
            <span class="keyword">for</span> i = 1:2
                subplot(2,1,i)
                lineHandles  =  stem(0:20, ACF(:,i) , <span class="string">'filled'</span> , <span class="string">'r-o'</span>);
                set(lineHandles , <span class="string">'MarkerSize'</span> , 4)
                grid(<span class="string">'on'</span>)
                xlabel(<span class="string">'Lag'</span>)
                ylabel(labs{i})
                hold(<span class="string">'on'</span>)
                plot([0.5 0.5 ; 20 20] , [bounds([1 1]) bounds([2 2])] , <span class="string">'-b'</span>);
                plot([0 20] , [0 0] , <span class="string">'-k'</span>);
                hold(<span class="string">'off'</span>)
                a  =  axis;
                axis([a(1:3) 1]);
            <span class="keyword">end</span>
</pre><p>The autocorrelation values at the first lag are significant for the intercept parameter, and even more so for the slope parameter.  We could repeat the sampling using a larger thinning parameter in order to reduce the correlation further.  For the purposes of this demo, however, we'll continue to use the current sample.</p><h2>Inference for the Model Parameters<a name="16"></a></h2><p>As expected, a histogram of the sample mimics the plot of the posterior density.</p><pre class="codeinput">            subplot(1,1,1)
            hist3(trace,[25,25]);
            xlabel(<span class="string">'Intercept'</span>)
            ylabel(<span class="string">'Slope'</span>)
            zlabel(<span class="string">'Posterior density'</span>)
            view(-110,30)
</pre><p>You can use a histogram or a kernel smoothing density estimate to summarize the marginal distribution properties of the posterior samples.</p><pre class="codeinput">            subplot(2,1,1)
            hist(trace(:,1))
            xlabel(<span class="string">'Intercept'</span>);
            subplot(2,1,2)
            ksdensity(trace(:,2))
            xlabel(<span class="string">'Slope'</span>);
</pre><p>You could also compute descriptive statistics such as the posterior mean or percentiles from the random samples. To determine if the sample size is large enough to achieve a desired precision, it is helpful to monitor the desired statistic of the traces as a function of the number of samples.</p><pre class="codeinput">            csum = cumsum(trace);
            subplot(2,1,1)
            plot(csum(:,1)'./(1:nsamples))
            xlabel(<span class="string">'Number of samples'</span>)
            ylabel(<span class="string">'Means of the intercept'</span>);
            subplot(2,1,2)
            plot(csum(:,2)'./(1:nsamples))
            xlabel(<span class="string">'Number of samples'</span>)
            ylabel(<span class="string">'Means of the slope'</span>);
</pre><p>In this case, it appears that the sample size of 1000 is more than sufficient to give good precision for the posterior mean estimate.</p><pre class="codeinput">        <span class="keyword">end</span>
 	<span class="keyword">end</span>
	<span class="comment">%  Created with Newcl by John J. Lee after newfcn by Frank Gonzalez-Morphy</span>
</pre><pre class="codeinput"> <span class="keyword">end</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
classdef Logistic < mlentropy.Bayesian
	%% LOGISTIC implements Bayesian inference with a logistic model:
    % 
    %  $$f(z) = \frac{e^z}{1 + e^z}, z \equiv B_0 + B_1 x_1 + B_2 x_2 + ...$$
    %  This is a generalized linear model. 
    % 
	%  Version $Revision$ was created $Date$ by $Author$  
 	%  and checked into svn repository $URL$ 
 	%  Developed on Matlab 7.10.0.499 (R2010a) 
 	%  $Id$ 
 	%  N.B. classdef (Sealed, Hidden, InferiorClasses = {?class1,?class2}, ConstructOnLoad) 

	properties 
 		% N.B. (Abstract, Access='private', GetAccess='protected', SetAccess='protected', ... 
 		%       Constant, Dependent, Hidden, Transient) 
        samples     % n x k   numeric arrays
        outcomes    % n x 1   numeric col. vector
        priors      % cell of function handles
        posterior   %         function handle
        model       %         function handle
        modelName = 'logistic'
        modelParams % cell of B0, B1, B2, ...
    end 
    
    properties (Dependent)
        nsamples
    end

	methods 

 		function this = Logistic(sample, outcome) 
 			%% LOGISTIC (ctor) 
 			%  Usage:  obj = Logistic(samples, binaryoutcomes)
			%                         ^        ^ numeric, dip_image, NIfTI, NIfTI filenames or cells of the same
			%                         n x k      n x 1
            this = this@mlentropy.Bayesian;
            import mlfourd.*;
            switch (class(sample))
                case 'char'
                    pnii = ensureNii(sample);
                    pdat = pnii.img;
                case NIfTI.NIFTI_SUBCLASS
                    pdat = sample.img;
                case NIfTI.NUMERIC_TYPES
                    pdat = double(sample);
                otherwise
                    paramError('mlentropy.Logistic.ctor', 'class(sample)', class(sample));
            end
            this.samples = pdat;
            switch (class(outcome))
                case 'char'
                    onii = ensureNii(outcome);
                    odat = onii.img;
                case NIfTI.NIFTI_SUBCLASS
                    odat = outcome.img;
                case NIfTI.NUMERIC_TYPES
                    odat = double(outcome);
                otherwise
                    paramError('mlentropy.Logistic.ctor', 'class(outcome)', class(outcome));
            end
            this.outcomes = odat;
            rand('state',0); randn('state',0);
 		end % Logistic (ctor) 
        
        function this = set.samples(this, s)
            this.samples = ensureNumeric(s, 'single');
        end
        
        function n = get.nsamples(this)
            n = size(this.samples,1);
        end
 	end 

	methods (Static)
 		% N.B. (Static, Abstract, Access=', Hidden, Sealed) 
        function demo
            %% Car Experiment Data
            % In some simple problems such as the previous normal mean inference
            % example, it is easy to figure out the posterior distribution in a closed
            % form. But in general problems that involve non-conjugate priors, the
            % posterior distributions are difficult or impossible to compute
            % analytically. We will consider logistic regression as an example. This
            % example involves an experiment to help model the proportion of cars of
            % various weights that fail a mileage test. The data include observations
            % of weight, number of cars tested, and number failed.  We will work with
            % a transformed version of the weights to reduce the correlation in our
            % estimates of the regression parameters.

            % A set of car weights
            weight = [2100 2300 2500 2700 2900 3100 3300 3500 3700 3900 4100 4300]';
            weight = (weight-2800)/1000;     % recenter and rescale
            % The number of cars tested at each weight
            total = [48 42 31 34 31 21 23 23 21 16 17 21]';
            % The number of cars that have poor mpg performances at each weight
            poor = [1 2 0 3 8 8 14 17 19 15 17 21]';
            
            obj = mlentropy.Logistic(weight, poor ./ total);
            
            %% Logistic Regression Model 
            % Logistic regression, a special case of a generalized linear model, 
            % is appropriate for these data since the response variable is binomial.
            % The logistic regression model can be written as:
            % 
            % $$P(\mathrm{failure}) = \frac{e^{Xb}}{1+e^{Xb}}$$
            % 
            % where X is the design matrix and b is the vector containing the model
            % parameters. In MATLAB(R), we can write this equation as:
            obj.model = @(b,x) exp(b(1)+b(2).*x)./(1+exp(b(1)+b(2).*x));

            %%
            % If you have some prior knowledge or some non-informative priors are
            % available, you could specify the prior probability distributions for the
            % model parameters. For example, in this demo, we use normal priors for the
            % intercept |b1| and slope |b2|, i.e.
            obj.priors    = cell(1,2);
            obj.priors{1} = @(b1) normpdf(b1,0,20);    % prior for intercept 
            obj.priors{2} = @(b2) normpdf(b2,0,20);    % prior for slope

            %%
            % By Bayes' theorem, the joint posterior distribution of the model parameters
            % is proportional to the product of the likelihood and priors. 
            obj.posterior = @(b) prod(binopdf(poor,total,obj.model(b,weight))) ...  % likelihood
                                 * obj.priors{1}(b(1)) * obj.priors{2}(b(2));       % priors

            %% Slice Sampling
            % Monte Carlo methods are often used in Bayesian data analysis to summarize
            % the posterior distribution. The idea is that, even if you cannot compute
            % the posterior distribution analytically, you can generate a random sample
            % from the distribution and use these random values to estimate the posterior
            % distribution or derived statistics such as the posterior mean, median,
            % standard deviation, etc. Slice sampling is an algorithm designed to sample
            % from a distribution with an arbitrary density function, known only up to a
            % constant of proportionality REPLACE_WITH_DASH_DASH exactly what is needed for sampling from a
            % complicated posterior distribution whose normalization constant is unknown.
            % The algorithm does not generate independent samples, but rather a Markovian
            % sequence whose stationary distribution is the target distribution.  Thus,
            % the slice sampler is a Markov Chain Monte Carlo (MCMC) algorithm.  However,
            % it differs from other well-known MCMC algorithms because only the scaled
            % posterior need be specified REPLACE_WITH_DASH_DASH no proposal or marginal distributions are
            % needed.
            %
            % This example shows how to use the slice sampler as part of a Bayesian
            % analysis of the mileage test logistic regression model, including generating
            % a random sample from the posterior distribution for the model parameters,
            % analyzing the output of the sampler, and making inferences about the model
            % parameters.  The first step is to generate a random sample.
            initial = [1 1];
            nsamples = 1000;
            trace = slicesample(initial,nsamples,'pdf',obj.posterior,'width',[20 2]);

            %% Analysis of Sampler Output
            % After obtaining a random sample from the slice sampler, it is important to
            % investigate issues such as convergence and mixing, to determine whether the
            % sample can reasonably be treated as a set of random realizations from the
            % target posterior distribution. Looking at marginal trace plots is the
            % simplest way to examine the output.
            subplot(2,1,1)
            plot(trace(:,1))
            ylabel('Intercept');
            subplot(2,1,2)
            plot(trace(:,2))
            ylabel('Slope');
            xlabel('Sample Number');

            %%
            % It is apparent from these plots is that the effects of the parameter
            % starting values take a while to disappear (perhaps fifty or so samples)
            % before the process begins to look stationary.
            %
            % It is also be helpful in checking for convergence to use a moving window
            % to compute statistics such as the sample mean, median, or standard
            % deviation for the sample.  This produces a smoother plot than the raw
            % sample traces, and can make it easier to identify and understand any
            % non-stationarity.
            movavg = filter( (1/50)*ones(50,1), 1, trace);
            subplot(2,1,1)
            plot(movavg(:,1))
            xlabel('Number of samples')
            ylabel('Means of the intercept');
            subplot(2,1,2)
            plot(movavg(:,2))
            xlabel('Number of samples')
            ylabel('Means of the slope');

            %%
            % Because these are moving averages over a window of 50 iterations, the first
            % 50 values are not comparable to the rest of the plot.  However, the
            % remainder of each plot seems to confirm that the parameter posterior means
            % have converged to stationarity after 100 or so iterations.  It is also
            % apparent that the two parameters are correlated with each other, in
            % agreement with the earlier plot of the posterior density.
            %
            % Since the settling-in period represents samples that can not reasonably be
            % treated as random realizations from the target distribution, it's probably
            % advisable not to use the first 50 or so values at the beginning of the
            % slice sampler's output.  You could just delete those rows of the output,
            % however, it's also possible to specify a "burn-in" period.  This is
            % convenient when a suitable burn-in length is already known, perhaps from
            % previous runs.
            trace = slicesample(initial,nsamples,'pdf',obj.posterior, ...
                                                 'width',[20 2],'burnin',50);
            subplot(2,1,1)
            plot(trace(:,1))
            ylabel('Intercept');
            subplot(2,1,2)
            plot(trace(:,2))
            ylabel('Slope');

            %%
            % These trace plots do not seem to show any non-stationarity, indicating that
            % the burn-in period has done its job.
            %
            % However, there is a second aspect of the trace plots that should also be
            % explored.  While the trace for the intercept looks like high frequency
            % noise, the trace for the slope appears to have a lower frequency
            % component, indicating there autocorrelation between values at adjacent
            % iterations.  We could still compute the mean from this autocorrelated
            % sample, but it is often convenient to reduce the storage requirements by
            % removing redundancy in the sample.  If this eliminated the
            % autocorrelation, it would also allow us to treat this as a sample of
            % independent values.  For example, you can thin out the sample by keeping
            % only every 10th value.
            trace = slicesample(initial,nsamples,'pdf',obj.posterior,'width',[20 2], ...
                                                            'burnin',50,'thin',10);

            %%
            % To check the effect of this thinning, it is useful to estimate the sample
            % autocorrelation functions from the traces and use them to check if the
            % samples mix rapidly.
            F    =  fft(detrend(trace,'constant'));
            F    =  F .* conj(F);
            ACF  =  ifft(F);
            ACF  =  ACF(1:21,:);                          % Retain lags up to 20.
            ACF  =  real([ACF(1:21,1) ./ ACF(1,1) ...
                         ACF(1:21,2) ./ ACF(1,2)]);       % Normalize.
            bounds  =  sqrt(1/nsamples) * [2 ; -2];       % 95% CI for iid normal

            labs = {'Sample ACF for intercept','Sample ACF for slope' };
            for i = 1:2
                subplot(2,1,i)
                lineHandles  =  stem(0:20, ACF(:,i) , 'filled' , 'r-o');
                set(lineHandles , 'MarkerSize' , 4)
                grid('on')
                xlabel('Lag')
                ylabel(labs{i})
                hold('on')
                plot([0.5 0.5 ; 20 20] , [bounds([1 1]) bounds([2 2])] , '-b');
                plot([0 20] , [0 0] , '-k');
                hold('off')
                a  =  axis;
                axis([a(1:3) 1]);
            end

            %%
            % The autocorrelation values at the first lag are significant for the
            % intercept parameter, and even more so for the slope parameter.  We could
            % repeat the sampling using a larger thinning parameter in order to reduce
            % the correlation further.  For the purposes of this demo, however, we'll
            % continue to use the current sample.

            %% Inference for the Model Parameters
            % As expected, a histogram of the sample mimics the plot of the posterior
            % density.
            subplot(1,1,1)
            hist3(trace,[25,25]);
            xlabel('Intercept')
            ylabel('Slope')
            zlabel('Posterior density')
            view(-110,30)

            %%
            % You can use a histogram or a kernel smoothing density estimate to summarize
            % the marginal distribution properties of the posterior samples.
            subplot(2,1,1)
            hist(trace(:,1))
            xlabel('Intercept');
            subplot(2,1,2)
            ksdensity(trace(:,2))
            xlabel('Slope');

            %%
            % You could also compute descriptive statistics such as the posterior mean or
            % percentiles from the random samples. To determine if the sample size is
            % large enough to achieve a desired precision, it is helpful to monitor the 
            % desired statistic of the traces as a function of the number of samples.
            csum = cumsum(trace);
            subplot(2,1,1)
            plot(csum(:,1)'./(1:nsamples))
            xlabel('Number of samples')
            ylabel('Means of the intercept');
            subplot(2,1,2)
            plot(csum(:,2)'./(1:nsamples))
            xlabel('Number of samples')
            ylabel('Means of the slope');
            %%
            % In this case, it appears that the sample size of 1000 is more than
            % sufficient to give good precision for the posterior mean estimate.
        end
 	end 
	%  Created with Newcl by John J. Lee after newfcn by Frank Gonzalez-Morphy 
 end 

##### SOURCE END #####
--></body></html>